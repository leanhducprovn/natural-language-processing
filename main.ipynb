{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fe87763",
      "metadata": {
        "id": "2fe87763"
      },
      "source": [
        "# Xử lý ngôn ngữ tự nhiên thống kê để phân tích cảm xúc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5876c4c",
      "metadata": {
        "id": "b5876c4c"
      },
      "source": [
        "## Làm sạch dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7395d10",
      "metadata": {
        "id": "a7395d10"
      },
      "source": [
        "Dữ liệu đầu vào"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "4822a9be",
      "metadata": {
        "id": "4822a9be"
      },
      "outputs": [],
      "source": [
        "raw_docs = [\n",
        "    \"Here are some very simple basic sentences.\", # Đây là một số câu cơ bản rất đơn giản.\n",
        "    \"They won’t be very interesting, I’m afraid.\", # Tôi e rằng chúng sẽ không thú vị cho lắm.\n",
        "    \"The point of these examples is to _learn how basic text \\ cleaning works_ on *very simple* data.\" # Mục đích của những ví dụ này là _tìm hiểu cách thức hoạt động của văn bản cơ bản \\ làm sạch_ trên dữ liệu *rất đơn giản*.\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9604d545",
      "metadata": {
        "id": "9604d545"
      },
      "source": [
        "Cài đặt NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "a6b224b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6b224b1",
        "outputId": "134e732a-ff22-407e-ad31-e4573565fdc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a28ee5",
      "metadata": {
        "id": "05a28ee5"
      },
      "source": [
        "Chuyển đổi dữ liệu dưới dạng chuỗi thành vectơ từ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "08f95f67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08f95f67",
        "outputId": "b15536cb-05ec-4986-ae3a-a7a785cc449d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', ',', 'I', '’', 'm', 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', '\\\\', 'cleaning', 'works_', 'on', '*', 'very', 'simple', '*', 'data', '.']]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
        "print(tokenized_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a56c79",
      "metadata": {
        "id": "03a56c79"
      },
      "source": [
        "Tìm kiếm trong dữ liệu các ký hiệu dấu câu, ký tự đặc biệt và loại bỏ chúng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "49680435",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "49680435",
        "outputId": "e8b1da46-4abc-4f0e-ef28-1e5129ae4ddc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "d9a83339",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9a83339",
        "outputId": "7036ec18-d2d1-4916-e5bf-aee246c6c040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', 'I', '’', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "tokenized_docs_no_punctuation = []\n",
        "for review in tokenized_docs:\n",
        "    new_review = []\n",
        "    for token in review:\n",
        "        new_token = regex.sub(u'', token)\n",
        "        if not new_token == u'':\n",
        "            new_review.append(new_token)\n",
        "    tokenized_docs_no_punctuation.append(new_review)\n",
        "print(tokenized_docs_no_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "040cabe2",
      "metadata": {
        "id": "040cabe2"
      },
      "source": [
        "Tạo gốc và xác định gốc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "a93626e5",
      "metadata": {
        "id": "a93626e5"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "# from nltk.stem.snowball import SnowballStemmer\n",
        "# from nltk.stem.wordnet import WordNetLemmatizer\n",
        "porter = PorterStemmer()\n",
        "# snowball = SnowballStemmer(\"english\")\n",
        "# wordnet = WordNetLemmatizer()\n",
        "# each of the following commands perform stemming on word\n",
        "# porter.stem(word)\n",
        "# snowball.stem(word)\n",
        "# wordnet.lemmatize(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9633dfc4",
      "metadata": {
        "id": "9633dfc4"
      },
      "source": [
        "Một quy trình làm sạch dữ liệu rất hữu ích khác bao gồm xóa các thẻ và thực thể HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "e0e4a3a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0e4a3a0",
        "outputId": "85fa9e95-2a13-468f-f0c0-54ae9753f5c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\n",
            "Cleaned text:\n",
            "While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "test_string =\"<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\"\n",
        "soup = BeautifulSoup(test_string, 'html.parser')\n",
        "print(\"Original text:\")\n",
        "print(test_string)\n",
        "print(\"Cleaned text:\")\n",
        "print(soup.get_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c96895a",
      "metadata": {
        "id": "4c96895a"
      },
      "source": [
        "## Trình bày văn bản"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790eba3f",
      "metadata": {
        "id": "790eba3f"
      },
      "source": [
        "Ví dụ về biểu diễn BoW cho hai văn bản"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "88a159e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a159e7",
        "outputId": "bcb9f25a-b8cb-4bed-9b2f-f6d86d81b274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_items([('Mireia', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Hector', 1)])\n",
            "dict_items([('Sergio', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Mireia', 1), ('loves', 1)])\n",
            "dict_items([('He', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('football', 1)])\n"
          ]
        }
      ],
      "source": [
        "mydoclist = [\n",
        "    'Mireia loves me more than Hector loves me',\n",
        "    'Sergio likes me more than Mireia loves me',\n",
        "    'He likes basketball more than football',\n",
        "]\n",
        "from collections import Counter\n",
        "for doc in mydoclist:\n",
        "    tf = Counter()\n",
        "    for word in doc.split():\n",
        "        tf[word] += 1\n",
        "    print(tf.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "0ab413a8",
      "metadata": {
        "id": "0ab413a8"
      },
      "outputs": [],
      "source": [
        "c = Counter() # a new, empty counter\n",
        "c = Counter('gallahad') # a new counter from an iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bfcfa8",
      "metadata": {
        "id": "93bfcfa8"
      },
      "source": [
        "Trả về số không cho các mục bị thiếu thay vì tăng Key Error (lỗi chính)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "f9410e31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9410e31",
        "outputId": "a6a90c7f-2d37-4219-83b1-e665c35589ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c = Counter(['eggs', 'ham'])\n",
        "c['bacon']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1980b793",
      "metadata": {
        "id": "1980b793"
      },
      "source": [
        "Một ví dụ để tính toán trình đánh giá tính năng dựa trên tần số từ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "759ec047",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "759ec047",
        "outputId": "35716795-c7e5-4a72-e501-50a11ceaea48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our vocabulary vector is [Mireia, than, Sergio, football, likes, loves, more, Hector, He, basketball, me]\n",
            "The doc is \"Mireia loves me more than Hector loves me\"\n",
            "The tf vector for Document 1 is [1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 2]\n",
            "The doc is \"Sergio likes me more than Mireia loves me\"\n",
            "The tf vector for Document 2 is [1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 2]\n",
            "The doc is \"He likes basketball more than football\"\n",
            "The tf vector for Document 3 is [0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0]\n",
            "All combined, here is our master document term matrix: \n",
            "[[1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 2], [1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 2], [0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0]]\n"
          ]
        }
      ],
      "source": [
        "def build_lexicon(corpus):\n",
        "    # define a set with all possible words included in all the sentences or \"corpus\"\n",
        "    # xác định một tập hợp với tất cả các từ có thể có trong tất cả các câu hoặc \"ngữ liệu\"\n",
        "    lexicon = set()\n",
        "    for doc in corpus:\n",
        "        lexicon.update([word for word in doc.split ()])\n",
        "    return lexicon\n",
        "def tf(term, document):\n",
        "    return freq(term, document)\n",
        "def freq(term, document):\n",
        "    return document.split().count(term)\n",
        "vocabulary = build_lexicon(mydoclist)\n",
        "doc_term_matrix = []\n",
        "print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
        "for doc in mydoclist:\n",
        "    print('The doc is \"' + doc + '\"')\n",
        "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
        "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
        "    print('The tf vector for Document %d is [%s]' % ((mydoclist.index(doc)+1), tf_vector_string))\n",
        "    doc_term_matrix.append(tf_vector)\n",
        "print(\"All combined, here is our master document term matrix: \")\n",
        "print(doc_term_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036f8bff",
      "metadata": {
        "id": "036f8bff"
      },
      "source": [
        "Cần thực hiện một số chuẩn hóa vectơ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "c3f22d9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3f22d9f",
        "outputId": "16e09eb7-5040-4e6f-d23b-ac7293ed0e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A regular old document term matrix:\n",
            "[[1 1 0 0 0 2 1 1 0 0 2]\n",
            " [1 1 1 0 1 1 1 0 0 0 2]\n",
            " [0 1 0 1 1 0 1 0 1 1 0]]\n",
            "\n",
            "A document term matrix with row-wise L2 norm:\n",
            "[[0.28867513 0.28867513 0.         0.         0.         0.57735027\n",
            "  0.28867513 0.28867513 0.         0.         0.57735027]\n",
            " [0.31622777 0.31622777 0.31622777 0.         0.31622777 0.31622777\n",
            "  0.31622777 0.         0.         0.         0.63245553]\n",
            " [0.         0.40824829 0.         0.40824829 0.40824829 0.\n",
            "  0.40824829 0.         0.40824829 0.40824829 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "def l2_normalizer(vec):\n",
        "    denom = np.sum([el**2 for el in vec])\n",
        "    return [(el / math.sqrt(denom)) for el in vec]\n",
        "doc_term_matrix_l2 = []\n",
        "for vec in doc_term_matrix:\n",
        "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
        "print('A regular old document term matrix:')\n",
        "print(np.matrix(doc_term_matrix))\n",
        "print('\\nA document term matrix with row-wise L2 norm:')\n",
        "print(np.matrix(doc_term_matrix_l2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5f4801",
      "metadata": {
        "id": "aa5f4801"
      },
      "source": [
        "Thử trọng số mỗi từ bằng tần số tài liệu nghịch đảo của nó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "1e05e2e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e05e2e2",
        "outputId": "824a5e9d-5dbd-4beb-8e44-512f279dccd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our vocabulary vector is [Mireia, than, Sergio, football, likes, loves, more, Hector, He, basketball, me]\n",
            "The inverse document frequency vector is [1.098612, 1.098612, 0.000000, 0.000000, 0.000000, 1.098612, 1.098612, 1.098612, 0.000000, 0.000000, 1.098612]\n"
          ]
        }
      ],
      "source": [
        "def numDocsContaining(word, doclist):\n",
        "    doccount = 0\n",
        "    for doc in doclist:\n",
        "        if freq(word, doc) > 0:\n",
        "            doccount += 1\n",
        "        return doccount\n",
        "def idf(word, doclist):\n",
        "    n_samples = len(doclist)\n",
        "    df = numDocsContaining(word, doclist)\n",
        "    return np.log(n_samples / df) if df != 0 else 0\n",
        "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
        "print('Our vocabulary vector is [' + ', '.join(list (vocabulary)) + ']')\n",
        "print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a44e41a",
      "metadata": {
        "id": "1a44e41a"
      },
      "source": [
        "Chuyển đổi vectơ IDF của chúng tôi thành một ma trận trong đó đường chéo là vectơ IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "9fc9f147",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fc9f147",
        "outputId": "63ef292c-fcf5-4363-de0e-3cb2db4f1dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.09861229 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         1.09861229 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         1.09861229\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  1.09861229 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         1.09861229 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.09861229]]\n"
          ]
        }
      ],
      "source": [
        "def build_idf_matrix(idf_vector):\n",
        "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
        "    np.fill_diagonal(idf_mat, idf_vector)\n",
        "    return idf_mat\n",
        "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
        "print(my_idf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f6fc616",
      "metadata": {
        "id": "8f6fc616"
      },
      "source": [
        "Chuẩn hóa từng tài liệu bằng cách sử dụng định mức L2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "005ae010",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "005ae010",
        "outputId": "4358df61-29ce-42ed-b184-acd670bb46a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Mireia', 'than', 'Sergio', 'football', 'likes', 'loves', 'more', 'Hector', 'He', 'basketball', 'me'}\n",
            "[[0.28867513 0.28867513 0.         0.         0.         0.57735027\n",
            "  0.28867513 0.28867513 0.         0.         0.57735027]\n",
            " [0.35355339 0.35355339 0.         0.         0.         0.35355339\n",
            "  0.35355339 0.         0.         0.         0.70710678]\n",
            " [0.         0.70710678 0.         0.         0.         0.\n",
            "  0.70710678 0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "doc_term_matrix_tfidf = []\n",
        "#performing tf-idf matrix multiplication\n",
        "for tf_vector in doc_term_matrix:\n",
        "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
        "#normalizing\n",
        "doc_term_matrix_tfidf_l2 = []\n",
        "for tf_vector in doc_term_matrix_tfidf:\n",
        "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
        "print(vocabulary)\n",
        "# np.matrix() just to make it easier to look at\n",
        "print(np.matrix(doc_term_matrix_tfidf_l2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c9311a",
      "metadata": {
        "id": "48c9311a"
      },
      "source": [
        "## Các trường hợp thực tế"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5797941e",
      "metadata": {
        "id": "5797941e"
      },
      "source": [
        "### Chuẩn bị dữ liệu và cài đặt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf8407f",
      "metadata": {
        "id": "1cf8407f"
      },
      "source": [
        "Cài đặt TextBlog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "1667b358",
      "metadata": {
        "id": "1667b358"
      },
      "outputs": [],
      "source": [
        "# Chạy lệnh dưới trên terminal\n",
        "# pip install -U textblob\n",
        "# python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30327618",
      "metadata": {
        "id": "30327618"
      },
      "source": [
        "Code ví dụ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "f73401f5",
      "metadata": {
        "id": "f73401f5"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import svm\n",
        "# from unidecode import unidecode\n",
        "def BoW(text):\n",
        "    # Tokenizing text\n",
        "    text_tokenized = [word_tokenize(doc) for doc in text]\n",
        "    # Removing punctuation\n",
        "    regex = re.compile('[%s]' % re.escape(string. punctuation))\n",
        "    tokenized_docs_no_punctuation = []\n",
        "    for review in text_tokenized:\n",
        "        new_review = []\n",
        "        for token in review:\n",
        "            new_token = regex.sub(u'', token)\n",
        "            if not new_token == u'':\n",
        "                new_review.append(new_token)\n",
        "        tokenized_docs_no_punctuation.append(new_review)\n",
        "    # Stemming and Lemmatizing\n",
        "    porter = PorterStemmer()\n",
        "    preprocessed_docs = []\n",
        "    for doc in tokenized_docs_no_punctuation:\n",
        "        final_doc = ''\n",
        "        for word in doc:\n",
        "            final_doc = final_doc + ' ' + porter. stem(word)\n",
        "        preprocessed_docs.append(final_doc)\n",
        "    return preprocessed_docs\n",
        "\n",
        "# read your train text data here\n",
        "# textTrain = ReadTrainDataText()\n",
        "# preprocessed_docs = BoW(textTrain) # for train data # Computing TIDF word space\n",
        "# tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
        "# trainData = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
        "# textTest = ReadTestDataText() #read your test text data here\n",
        "# prepro_docs_test = BoW(textTest) # for test data testData = tfidf_vectorizer.transform(prepro_docs_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5abe7f54",
      "metadata": {
        "id": "5abe7f54"
      },
      "source": [
        "### Phân tích tình cảm khi đánh giá phim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17f122dd",
      "metadata": {
        "id": "17f122dd"
      },
      "source": [
        "#### Import module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "b05adbe7",
      "metadata": {
        "id": "b05adbe7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YLg9d5v1azf5",
      "metadata": {
        "id": "YLg9d5v1azf5"
      },
      "source": [
        "#### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "iRpjuqPLbDv4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "iRpjuqPLbDv4",
        "outputId": "5675c887-502c-4f5f-cee1-339d70938cdd"
      },
      "outputs": [],
      "source": [
        "reviews_train = []\n",
        "for line in open('./aclImdb/movie_data/full_train.txt', 'r'):\n",
        "    \n",
        "    reviews_train.append(line.strip())\n",
        "    \n",
        "reviews_test = []\n",
        "for line in open('./aclImdb/movie_data/full_test.txt', 'r'):\n",
        "    \n",
        "    reviews_test.append(line.strip())\n",
        "    \n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a4d002",
      "metadata": {},
      "source": [
        "Reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "078a84c3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...<br /><br />Writer Armistead Maupin and director Patrick Stettner have truly succeeded! <br /><br />With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle with issues of trust both in his personnel life(Jess) and the world around him(Donna).<br /><br />As we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives irrevocably. The request to review a book written by a young man turns into a life changing event that helps Gabriel find the strength within himself to carry on and move forward.<br /><br />It\\'s to bad that most people will avoid this film. I only say that because the average American will probably think \"Robin Williams in a serious role? That didn\\'t work before!\" PLEASE GIVE THIS MOVIE A CHANCE! Robin Williams touches the darkness we all must find and go through in ourselves to be better people. Like his movie One Hour Photo he has stepped up as an actor and made another quality piece of art.<br /><br />Oh and before I forget, I believe Bobby Cannavale as Jess steals every scene he is in. He has the 1940\\'s leading man looks and screen presence. It\\'s this hacks opinion he could carry his own movie right now!!<br /><br />S~'"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_train[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c265d679",
      "metadata": {},
      "source": [
        "#### Làm sạch và tiền xử lý"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "d192ac5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "NO_SPACE = \"\"\n",
        "SPACE = \" \"\n",
        "\n",
        "def preprocess_reviews(reviews):\n",
        "    \n",
        "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
        "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
        "    \n",
        "    return reviews\n",
        "\n",
        "reviews_train_clean = preprocess_reviews(reviews_train)\n",
        "reviews_test_clean = preprocess_reviews(reviews_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8df807",
      "metadata": {},
      "source": [
        "Reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "a37c1396",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'when i first read armistead maupins story i was taken in by the human drama displayed by gabriel no one and those he cares about and loves that being said we have now been given the film version of an excellent story and are expected to see past the gloss of hollywood writer armistead maupin and director patrick stettner have truly succeeded  with just the right amount of restraint robin williams captures the fragile essence of gabriel and lets us see his struggle with issues of trust both in his personnel lifejess and the world around himdonna as we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives irrevocably the request to review a book written by a young man turns into a life changing event that helps gabriel find the strength within himself to carry on and move forward its to bad that most people will avoid this film i only say that because the average american will probably think robin williams in a serious role that didnt work before please give this movie a chance robin williams touches the darkness we all must find and go through in ourselves to be better people like his movie one hour photo he has stepped up as an actor and made another quality piece of art oh and before i forget i believe bobby cannavale as jess steals every scene he is in he has the s leading man looks and screen presence its this hacks opinion he could carry his own movie right now s~'"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_train_clean[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efdbeeb8",
      "metadata": {},
      "source": [
        "Vectơ hóa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "bb1e08ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(reviews_train_clean)\n",
        "X = cv.transform(reviews_train_clean)\n",
        "X_test = cv.transform(reviews_test_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b2dde8",
      "metadata": {},
      "source": [
        "#### Xây dựng bộ phân loại"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "5440823a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.87024\n",
            "Accuracy for C=0.05: 0.88272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.25: 0.87808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.5: 0.87392\n",
            "Accuracy for C=1: 0.87088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    lr = LogisticRegression(C=c)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print(\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_val, lr.predict(X_val))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d058b7eb",
      "metadata": {},
      "source": [
        "Mô hình cuối cùng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "39be501f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Accuracy: 0.88144\n"
          ]
        }
      ],
      "source": [
        "final_model = LogisticRegression(C = 0.05)\n",
        "final_model.fit(X, target)\n",
        "print(\"Final Accuracy: %s\" % accuracy_score(target, final_model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742ba793",
      "metadata": {},
      "source": [
        "Xem xét các hệ số lớn nhất và nhỏ nhất, tương ứng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "a6f774f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('excellent', 0.9283544427618067)\n",
            "('perfect', 0.7944277472574596)\n",
            "('great', 0.6745553291414698)\n",
            "('amazing', 0.6164834476214972)\n",
            "('superb', 0.6055919831751899)\n",
            "('worst', -1.3679897533477545)\n",
            "('waste', -1.1688808995878297)\n",
            "('awful', -1.0273337384921135)\n",
            "('poorly', -0.8748022417025753)\n",
            "('boring', -0.8591221031199628)\n"
          ]
        }
      ],
      "source": [
        "feature_to_coef = {\n",
        "    word: coef for word, coef in zip(\n",
        "        cv.get_feature_names(), final_model.coef_[0]\n",
        "    )\n",
        "}\n",
        "for best_positive in sorted(\n",
        "    feature_to_coef.items(), \n",
        "    key = lambda x: x[1], \n",
        "    reverse = True)[:5]:\n",
        "    print (best_positive)\n",
        "    \n",
        "#     ('excellent', 0.9288812418118644)\n",
        "#     ('perfect', 0.7934641227980576)\n",
        "#     ('great', 0.675040909917553)\n",
        "#     ('amazing', 0.6160398142631545)\n",
        "#     ('superb', 0.6063967799425831)\n",
        "    \n",
        "for best_negative in sorted(\n",
        "    feature_to_coef.items(), \n",
        "    key = lambda x: x[1])[:5]:\n",
        "    print (best_negative)\n",
        "    \n",
        "#     ('worst', -1.367978497228895)\n",
        "#     ('waste', -1.1684451288279047)\n",
        "#     ('awful', -1.0277001734353677)\n",
        "#     ('poorly', -0.8748317895742782)\n",
        "#     ('boring', -0.8587249740682945)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b5876c4c",
        "4c96895a",
        "5797941e"
      ],
      "include_colab_link": true,
      "name": "natural-language-processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
