{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe87763",
   "metadata": {},
   "source": [
    "# Xử lý ngôn ngữ tự nhiên thống kê để phân tích cảm xúc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5876c4c",
   "metadata": {},
   "source": [
    "## Làm sạch dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7395d10",
   "metadata": {},
   "source": [
    "Dữ liệu đầu vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4822a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\n",
    "    \"Here are some very simple basic sentences.\", # Đây là một số câu cơ bản rất đơn giản.\n",
    "    \"They won’t be very interesting, I’m afraid.\", # Tôi e rằng chúng sẽ không thú vị cho lắm.\n",
    "    \"The point of these examples is to _learn how basic text \\ cleaning works_ on *very simple* data.\" # Mục đích của những ví dụ này là _tìm hiểu cách thức hoạt động của văn bản cơ bản \\ làm sạch_ trên dữ liệu *rất đơn giản*.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604d545",
   "metadata": {},
   "source": [
    "Cài đặt NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a6b224b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a28ee5",
   "metadata": {},
   "source": [
    "Chuyển đổi dữ liệu dưới dạng chuỗi thành vectơ từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "08f95f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', ',', 'I', '’', 'm', 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', '\\\\', 'cleaning', 'works_', 'on', '*', 'very', 'simple', '*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a56c79",
   "metadata": {},
   "source": [
    "Tìm kiếm trong dữ liệu các ký hiệu dấu câu, ký tự đặc biệt và loại bỏ chúng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "49680435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d9a83339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', 'I', '’', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cabe2",
   "metadata": {},
   "source": [
    "Tạo gốc và xác định gốc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a93626e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "porter = PorterStemmer()\n",
    "# snowball = SnowballStemmer(\"english\")\n",
    "# wordnet = WordNetLemmatizer()\n",
    "# each of the following commands perform stemming on word\n",
    "# porter.stem(word)\n",
    "# snowball.stem(word)\n",
    "# wordnet.lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633dfc4",
   "metadata": {},
   "source": [
    "Một quy trình làm sạch dữ liệu rất hữu ích khác bao gồm xóa các thẻ và thực thể HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0e4a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\n",
      "Cleaned text:\n",
      "While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "test_string =\"<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\"\n",
    "soup = BeautifulSoup(test_string, 'html.parser')\n",
    "print(\"Original text:\")\n",
    "print(test_string)\n",
    "print(\"Cleaned text:\")\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96895a",
   "metadata": {},
   "source": [
    "## Trình bày văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790eba3f",
   "metadata": {},
   "source": [
    "Ví dụ về biểu diễn BoW cho hai văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "88a159e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Mireia', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Hector', 1)])\n",
      "dict_items([('Sergio', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Mireia', 1), ('loves', 1)])\n",
      "dict_items([('He', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('football', 1)])\n"
     ]
    }
   ],
   "source": [
    "mydoclist = [\n",
    "    'Mireia loves me more than Hector loves me',\n",
    "    'Sergio likes me more than Mireia loves me',\n",
    "    'He likes basketball more than football',\n",
    "]\n",
    "from collections import Counter\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] += 1\n",
    "    print(tf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0ab413a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter() # a new, empty counter\n",
    "c = Counter('gallahad') # a new counter from an iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bfcfa8",
   "metadata": {},
   "source": [
    "Trả về số không cho các mục bị thiếu thay vì tăng Key Error (lỗi chính)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f9410e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(['eggs', 'ham'])\n",
    "c['bacon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980b793",
   "metadata": {},
   "source": [
    "Một ví dụ để tính toán trình đánh giá tính năng dựa trên tần số từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "759ec047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [Hector, football, He, likes, more, loves, basketball, Mireia, than, Sergio, me]\n",
      "The doc is \"Mireia loves me more than Hector loves me\"\n",
      "The tf vector for Document 1 is [1, 0, 0, 0, 1, 2, 0, 1, 1, 0, 2]\n",
      "The doc is \"Sergio likes me more than Mireia loves me\"\n",
      "The tf vector for Document 2 is [0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 2]\n",
      "The doc is \"He likes basketball more than football\"\n",
      "The tf vector for Document 3 is [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
      "All combined, here is our master document term matrix: \n",
      "[[1, 0, 0, 0, 1, 2, 0, 1, 1, 0, 2], [0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 2], [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def build_lexicon(corpus):\n",
    "    # define a set with all possible words included in all the sentences or \"corpus\"\n",
    "    # xác định một tập hợp với tất cả các từ có thể có trong tất cả các câu hoặc \"ngữ liệu\"\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split ()])\n",
    "    return lexicon\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "vocabulary = build_lexicon(mydoclist)\n",
    "doc_term_matrix = []\n",
    "print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "for doc in mydoclist:\n",
    "    print('The doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    print('The tf vector for Document %d is [%s]' % ((mydoclist.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "print(\"All combined, here is our master document term matrix: \")\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f8bff",
   "metadata": {},
   "source": [
    "Cần thực hiện một số chuẩn hóa vectơ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c3f22d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old document term matrix:\n",
      "[[1 0 0 0 1 2 0 1 1 0 2]\n",
      " [0 0 0 1 1 1 0 1 1 1 2]\n",
      " [0 1 1 1 1 0 1 0 1 0 0]]\n",
      "\n",
      "A document term matrix with row-wise L2 norm:\n",
      "[[0.28867513 0.         0.         0.         0.28867513 0.57735027\n",
      "  0.         0.28867513 0.28867513 0.         0.57735027]\n",
      " [0.         0.         0.         0.31622777 0.31622777 0.31622777\n",
      "  0.         0.31622777 0.31622777 0.31622777 0.63245553]\n",
      " [0.         0.40824829 0.40824829 0.40824829 0.40824829 0.\n",
      "  0.40824829 0.         0.40824829 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "print('A regular old document term matrix:')\n",
    "print(np.matrix(doc_term_matrix))\n",
    "print('\\nA document term matrix with row-wise L2 norm:')\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f4801",
   "metadata": {},
   "source": [
    "Thử trọng số mỗi từ bằng tần số tài liệu nghịch đảo của nó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1e05e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [Hector, football, He, likes, more, loves, basketball, Mireia, than, Sergio, me]\n",
      "The inverse document frequency vector is [1.098612, 0.000000, 0.000000, 0.000000, 1.098612, 1.098612, 0.000000, 1.098612, 1.098612, 0.000000, 1.098612]\n"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount += 1\n",
    "        return doccount\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / df) if df != 0 else 0\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "print('Our vocabulary vector is [' + ', '.join(list (vocabulary)) + ']')\n",
    "print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44e41a",
   "metadata": {},
   "source": [
    "Chuyển đổi vectơ IDF của chúng tôi thành một ma trận trong đó đường chéo là vectơ IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9fc9f147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.09861229 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.09861229 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.09861229\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.09861229 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.09861229 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.09861229]]\n"
     ]
    }
   ],
   "source": [
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "print(my_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fc616",
   "metadata": {},
   "source": [
    "Chuẩn hóa từng tài liệu bằng cách sử dụng định mức L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "005ae010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hector', 'football', 'He', 'likes', 'more', 'loves', 'basketball', 'Mireia', 'than', 'Sergio', 'me'}\n",
      "[[0.28867513 0.         0.         0.         0.28867513 0.57735027\n",
      "  0.         0.28867513 0.28867513 0.         0.57735027]\n",
      " [0.         0.         0.         0.         0.35355339 0.35355339\n",
      "  0.         0.35355339 0.35355339 0.         0.70710678]\n",
      " [0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.70710678 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tfidf = []\n",
    "#performing tf-idf matrix multiplication\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "#normalizing\n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "print(vocabulary)\n",
    "# np.matrix() just to make it easier to look at\n",
    "print(np.matrix(doc_term_matrix_tfidf_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9311a",
   "metadata": {},
   "source": [
    "## Các trường hợp thực tế"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f73401f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReadTrainDataText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3r/5py9qw1j223_tkd90p3752340000gn/T/ipykernel_10166/651958405.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# read your train text data here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtextTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadTrainDataText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mpreprocessed_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextTrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for train data # Computing TIDF word space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ReadTrainDataText' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from unidecode import unidecode\n",
    "def BoW(text):\n",
    "    # Tokenizing text\n",
    "    text_tokenized = [word_tokenize(doc) for doc in text]\n",
    "    # Removing punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string. punctuation))\n",
    "    tokenized_docs_no_punctuation = []\n",
    "    for review in text_tokenized:\n",
    "        new_review = []\n",
    "        for token in review:\n",
    "            new_token = regex.sub(u'', token)\n",
    "            if not new_token == u'':\n",
    "                new_review.append(new_token)\n",
    "        tokenized_docs_no_punctuation.append(new_review)\n",
    "    # Stemming and Lemmatizing\n",
    "    porter = PorterStemmer()\n",
    "    preprocessed_docs = []\n",
    "    for doc in tokenized_docs_no_punctuation:\n",
    "        final_doc = ''\n",
    "        for word in doc:\n",
    "            final_doc = final_doc + ' ' + porter. stem(word)\n",
    "        preprocessed_docs.append(final_doc)\n",
    "    return preprocessed_docs\n",
    "\n",
    "# read your train text data here\n",
    "textTrain = ReadTrainDataText()\n",
    "preprocessed_docs = BoW(textTrain) # for train data # Computing TIDF word space\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "trainData = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "textTest = ReadTestDataText() #read your test text data here\n",
    "prepro_docs_test = BoW(textTest) # for test data testData = tfidf_vectorizer.transform(prepro_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec052dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
